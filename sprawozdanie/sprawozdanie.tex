\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{polski}
\usepackage{amsmath}
\usepackage{a4wide}
\usepackage{url}
\pagestyle{plain}

\makeatletter
  \renewcommand{\@seccntformat}[1]{\csname the#1\endcsname.\quad}
\makeatother

\title{\textbf{Wyszukiwanie informacji} \\ pracownia}
\author{
Wiktor Janas \thanks{\texttt{wixorpeek@gmail.com}}
\and
Mateusz Dereniowski \thanks{\texttt{Mateusz.Dereniowski@gmail.com}}}

\begin{document}

\maketitle
\thispagestyle{empty}

\tableofcontents

\newpage

\section{O projekcie}
Niniejsza praca dokumentuje projekt polegający na stworzeniu wyszukiwarki dla
polskiej Wikipedii. Projekt obejmuje stworzenie indeksera, czyli programu
przetwarzającego tekst Wikipedii do postaci ułatwiającej wyszukiwanie, oraz
samej wyszukiwarki, czyli programu odpowiadającego na zapytania.

Wyszukiwarka obsługuje zapytania boolowskie, frazowe oraz tekstowe z rankingowaniem
artykułów (metodą tf-idf + pagerank). Dla zapytań boolowskich oraz tekstowych
stosowana jest lematyzacja. W realizacji projektu wykorzystano udostępniony
tekst Wikipedii\footnote{\url{http://www.ii.uni.wroc.pl/~prych/IR/Wikipedia_dla_wyszukiwarek.txt.gz}},
morfologik\footnote{\url{http://www.ii.uni.wroc.pl/~prych/text_mining/bazy_do_tm.txt}},
listę \textit{stopwords}\footnote{\url{http://www.ii.uni.wroc.pl/~prych/text_mining/stop_words.txt}},
listę linków między artykułami\footnote{\url{http://www.ii.uni.wroc.pl/~prych/IR/Wikilinki.txt.gz}}
oraz listę ciekawych artykułów\footnote{\url{https://kno.ii.uni.wroc.pl/ii/file.php/140/interesujace_artykulu.txt}}.

Projekt został zrealizowany przy użyciu języków C++ oraz Python. Wymagany jest
system operacyjny Linux wraz z biblioteką \texttt{talloc} (zazwyczaj wystarczy
zainstalować pakiet \texttt{libtalloc-dev}). Aby skompilować programy należy
wydać polecenie \texttt{make} w katalogu z kodem źródłowym. Następnie można
przystąpić do tworzenia indeksu: w tem celu należy wydać polecenie
\texttt{make the-index}. Skrypt oczekuje położenia danych wejściowych w katalogu
\texttt{Wiki\_dane} zgodnie z zapisami w pliku Makefile; zapisy te należy
dostosować do rzeczywistej lokalizacji danych wejściowych. Należy zwrócić uwagę
na kodowanie znaków w plikach; wymagane jest kodowanie UTF-8 oraz końce linii 
w stylu UNIX; spośród dostarczonych materiałów plik \textit{stopwords} wymaga
konwersji końców linii (można jej dokonać używając programu \texttt{fromdos}),
zaś plik \texttt{pytania.txt} nie jest poprawnym plikiem UTF-8. Wraz z projektem
dostarczony jest utworzony indeks. Wreszcie można uruchomić wyszukiwarkę wydając
polecenie \texttt{./search}. Wyszukiwarka domyślnie pracuje w trybie wsadowym:
wczytuje zapytania ze standardowego wejścia i wypisuje wyniki na standardowe
wyjście; można ją zatem przetestować używając polecenia \texttt{./search < queries.txt > answers.txt}.
Wyszukiwarkę można wywołać z następującymi opcjami:\begin{itemize}
  \item \texttt{-v}: tryb szczegółowy, wyszukiwarka wypisuje zachętę do
  wprowadzenia zapytania oraz wiele informacji o przetwarzaniu;
  \item \texttt{-r}: blokada wypisywania wyników, przydatna dla zapytań
  boolowskich oraz frazowych gdy lista wyników jest długa;
  \item \texttt{-m}: ograniczenie zbioru artykułów do ,,ciekawych artykułów''
  (tylko dla zapytań tekstowych);
  \item \texttt{-b $n$}: wypisywanie tylko $n$ najlepszch artykułów
  (tylko dla zapytań tekstowych);
  \item \texttt{-f $\alpha$:$\beta$:$\gamma$}: konfiguracja parametrów
  rankingowania artykułów (tylko dla zapytań tekstowych);
  \item \texttt{-h}: informacja o dostępnych opcjach.
\end{itemize}

\section{Ciekawe rozwiązania zastosowane w projekcie}
Wśród wielu standardowych rozwiązań zastosowanych w projekcie (parser zapytań,
algorytmy obliczania sumy i iloczynu list postingowych) znalazły się też takie,
które wydają się nam szczególnie ciekawe. Zostały one opisane poniżej.

\subsection{Zamiana słów na liczby}
Rozwiązanie problemu zamiany słów na liczby jest podstawową kwestią wymagającą
rozwiązania w wyszukiwarce: operacje na słowach są nieefektywne, słowami nie
można na przykład indeksować tablicy (,,tablica'' umożliwiająca taką operację
jest w istocie słownikiem i czas dostępu do niej często jest niezadowalający).
Z jednej strony wejściem dla indeksera jest wielki zbiór słów, z drugiej strony
wyszukiwarka powinna możliwie błyskawicznie parsować zapytanie.

Standardowym podejściem do problemu byłoby posortowanie wszystkich znanych słów
leksykograficznie, a następnie zastosowanie wyszukiwania binarnego w celu
odnalezienia indeksu poszukiwanego słowa w tablicy. Inną (równoważną)
możliwością byłoby umieszczenie słów w zbalansowanym drzewie poszukiwań
binarnych. Oba te podejścia charakteryzuje jednak logarytmiczny czas dostępu do
słowa (względem ilości słów). Ponadto wymagają one dość chaotycznych dostępów
do pamięci, co redukuje efektywność cache'a procesora. Uzyskanie stałego czasu
dostępu byłoby możliwe z wykorzystaniem struktury trie, jest ona jednak bardzo
pamięciożerna.

Rozwiązanie zastosowane w tej pracy opiera się na tabeli hashującej.
Podczas budowania indeksu wszystkie słowa są hashowane i dzielone na kubełki.
W kubełku ustalana jest arbitralna kolejność słów. Następnie wszystkie kubełki
są konkatenowane: w ten sposób każde słowo otrzymuje swój unikalny
identyfikator z przedziału od zera do $\text{ilości słów} - 1$. Wyszukiwanie
numeru słowa polega na obliczeniu jego hasha oraz odnalezienia numeru szukanego
słowa w kubełku. Aby obliczyć identyfikator wystarczy numer ten dodać do
łącznej ilości słów we wszystkich poprzedzających kubełkach (wielkość tą
oblicza się prostym preprocessingiem i zapamiętuje).

\subsection{Lematyzacja}
Celem lematyzacji jest uniezależnienie wyników zapytania od form gramatycznych
użytych przez użytkownika i autora tekstu. W związku z tym, w odpowiedzi na term
wpisany przez użytkownika chcemy zwrócić dokument, w którym występuje
jakiekolwiek słowo, które ma wspólną formę bazową z termem z zapytania. Prostą
realizacją tego pomysłu byłoby: zindeksuj każdy term z Wikipedii jako wszystkie
jego formy bazowe, zaś przy wykonywaniu zapytania potraktuj term z zapytania
jako sumę (or) wszystkich jego form bazowych. Niestety takie podejście narzuca
na wyszukiwarkę konieczność zamienienia każdego termu na odpowiednią sumę,
obliczania tych sum oraz, co najgorsze, odczytywania z dysku wielu list
postingowych.

W niniejszej pracy zastosowano inne rozwiązanie. Przypuśćmy, że dla każdego
termu znajdujemy wszystkie jego formy bazowe, zaś dla każdej z nich znajdujemy
z kolei wszystkie formy pochodne. Otrzymane zbiory sumujemy i term początkowy
indeksujemy jako każdy z termów z wynikowej sumy. Na przykład dla termu
,,kota'' formy bazowe to ,,kot'' oraz ,,kota''. Pochodnymi słowa ,,kot'' są
,,kocie'', ,,kotowi'', ,,kotów'' itd., zaś pochodnymi słowa ,,kota'' są
na przykład ,,kotą'' oraz ,,koto''. Zatem słowo ,,kota'' zostałoby zindeksowane
jako każde z powyższych słów.

Opisane powyżej podejście zwalnia wyszukiwarkę z obowiązku sprawdzania form
bazowych w morfologiku oraz obliczania sum; niesie jednak ze sobą koszt
niebagatelnego zwiększenia rozmiaru słownika: zwykły rzeczownik odmienia się
przecież przez siedem przypadków i dwie liczby, zatem zostałby zindeksowany
przynajmniej czternaście razy! Problem ten jednak da się rozwiązać korzystając
z następującej obserwacji: prawie wszystkie formy pochodne jednego rzeczownika
mają tylko jedną formę bazową. Zatem wszystkie one będą miały dokładnie te same
wpisy na listach lematyzowanych. Można zatem pamiętać tylko jedną taką listę
i zapisać w indeksie, że wszystkie formy tego rzeczownika współdzielą ją.
Rozwiązanie to nie usuwa całkowicie problemu duplikacji: w morfologiku może
wystąpić sytuacja, gdzie bazą dla \texttt{a} jest \texttt{A}, zaś bazą dla
\texttt{b} jest \texttt{A} oraz \texttt{B}. W takim razie wystąpienie termu
\texttt{a} powinno zostać zindeksowane jako wystąpienie \texttt{a} oraz
\texttt{b}, jednak wystąpienie \texttt{b} liczy się jedynie jako wystąpienie
\texttt{b}. Widać zatem, że nie każdy term zostanie zindeksowany dokładnie raz.
Nie stanowi to jednak poważnego problemu: łatwo zauważyć, że sytuacja taka może
wystąpić jedynie w przypadku słów, które mają przynajmniej dwie formy bazowe,
słów takich jest zaś niewiele. Dzięki powyższej optymalizacji rozmiar indeksu
nie rośnie istotnie, zaś wyszukiwarka nie musi martwić się lematyzacją.

\subsection{Wczytywanie list w tle}
Najwolniejszą operacją w systemie komputerowym jest przesuwanie głowicy
dysku. Żądania odczytu generowane przez wyszukiwarkę są niestety bardzo
rozproszone (listy pozycyjne odpowiadające termom w zapytaniu prawie
nigdy nie znajdują się obok siebie). Przeto przez lwią część czasu
wyszukiwarka oczekuje na dane z dysku. Jest to okazja do optymalizacji:
wyszukiwarka może zlecić wykonanie odczytów innemu wątkowi; kiedy tylko
dostarczy on jakąś listę pozycyjną może ona przystąpić do jej przetwarzania,
zaś ów wątek może bez zwłoki zażądać od systemu odczytu kolejnej listy
postingowej. Ponieważ czas przetwarzania listy znacznie dominuje nad czasem
odczytu z dysku, powinno dać się go ,,ukryć'' tak, aby nie miał wpływu na
całkowity czas wykonywania zapytania.

W wyszukiwarce zastosowano opisaną powyżej optymalizację. Okazuje się jednak,
że jej wpływ nie jest znaczący. Częściowo jest to spowodowane tym, że wiele
zapytań składa się z dwóch lub trzech słów. Ponieważ główny wątek potrzebuje
przynajmniej dwóch list, aby wykonać jakieś obliczenia, w przypadku krótkich
zapytań może okazać się, że przetwarzanie można rozpocząć dopiero po odczytaniu
z dysku wszystkich zażądanych list.

Inną możliwą do wykonania optymalizacją jest zażądanie od systemu odczytu
\textit{wielu} list jednocześnie. Należy przypuszczać, że spowodowałoby
to znaczny spadek czasu odczytu z dysku, gdyż system mógłby wysłać wiele
żądań do dysku, ten zaś mógłby zoptymalizować trasę głowicy nad talerzami
(mechanizmy takie istnieją i są wykorzystywane w zaawansowanych aplikacjach).
Tej optymalizacji jednak nie zaimplementowano.

\subsection{Optymalizator}
Przed przystąpieniem do wykonywania zapytania boolowskiego uruchamiany jest
na nim optymalizator. Operuje on na drzewie zapytania i potrafi dokonać
następujących przekształceń:
\begin{itemize}
 \item usuwanie termów o pustych listach postingowych (jeśli term taki
 pojawi się w klauzuli AND, cała klauzula jest od razu zamieniana na
 pustą listę postingową; w klauzuli OR term jest po prostu usuwany),
 \item usuwanie \textit{stopwords} (\textit{stopwords} są usuwane jedynie
 z klauzul AND oraz jedynie gdy ich ilość nie przekracza połowy ilości
 termów w zapytaniu),
 \item sklejanie takich samych termów (wykonywanie operacji AND lub
 OR na takich samych termach nie ma sensu, zduplikowane wystąpienia 
 termu są usuwane; porównywanie dokonywane jest przy użyciu identyfikatora
 listy postingowej; ze względu na mechanizm lematyzacji termy w zapytaniu
 \texttt{kotu|kotom} mają ten sam identyfikator i jeden z nich zostanie
 usunięty),
 \item obliczanie optymalnej kolejności łączenia list (używany jest
 algorytm Huffmana, przy czym dla operacji OR przyjmuje się, że
 rozmiar wyniku będzie sumą rozmiarów danych wejściowych, zaś dla
 klauzuli AND -- minimum).
\end{itemize}
Dla zapytań frazowych optymalizator znajduje jedynie optymalną kolejność
łączenia list: od najkrótszych do najdłuższych.
Pomimo wykonania powyższych optymalizacji, w zapytaniu wciąż mogą pojawić
się wielokrotnie te same listy postingowe, na przykład w zapytaniu
\texttt{(pies kot) | (słoń pies|kot)}. Znajdowany jest więc zbiór
unikalnych list postingowych tak, aby każdą listę odczytać z dysku
tylko raz.

\subsection{Praca na całej Wikipedii}
Zrealizowany projekt umożliwa prace na całym dostarczonym zbiorze Wikipedii,
nie jest koniecznie (ani możliwe) wstępne odsiewanie artykułów. Dodanie obsługi
zapytań tekstowych z rankingowaniem spowodowało jedynie niewielkie wydłużenie
czasu tworzenia indeksu i nie niesie praktycznie żadnego nakładu czasowego przy
realizacji zapytań boolowskich lub frazowych. Jednak w celu zapewnienia
porównywalności wyników wyszukiwarka została zaopatrzona w funkcję przycinania
wyników do zbioru ,,ciekawych artykułów''. Opcja ta jest aktywowana
przełącznikiem z linii poleceń.

\section{Zapytania tekstowe z rankingowaniem}
Wyszukiwarka rozpoznaje zapytania w postaci $t1$ $t2$ $t3$ $\dots$ jako
zapytania tekstowe (w poprzedniej wersji tej pracy zapytanie takiej postaci było
traktowane jako zapytanie boolowskie ze spójnikami \texttt{AND}). W celu jego
realizacji wyszukiwarka odnajduje listy lematyzowane wszystkich termów zawartych
w zapytaniu oraz oblicza ich sumę (tak, jakby termy połączone były spójnikami
\texttt{OR}). Jest to uzasadnione, gdyż nieobecność jednego z termów w artykule
nie powinna dyskwalifikować go z obecności w wynikach wyszukiwania.

Podczas obliczania sumy list postingowych każdy artykuł otrzymuje również
pewną wagę w modelu przestrzeni wektorowej. Jeżeli potraktujemy artykuł (bądź
zapytanie) jako zbiór par (term,pozycja) oraz oznaczymy zbiór wszystkich
dokumentów przez $D$ oraz zapytanie przez $q$, możemy zdefiniować:
\newcommand{\simi}[2]{\mathrm{sim}(#1,#2)}
\newcommand{\idf}[1]{\mathrm{idf}_{#1}}
\newcommand{\tf}[2]{\mathrm{tf}_{#1,#2}}
\[ \idf{t} = \log \frac{ |D| }{ |\{d \in D \mid \exists p \;.\; \langle t,p \rangle \in d\}| } \]
\[ \tf{t}{d} = \frac{ |\{ p \mid \langle t,p \rangle \in d\}| }{ |d| } \]
\[ \vec v_d = \begin{bmatrix} \tf{t_0}{d} \cdot \idf{t_0},\;
                \tf{t_1}{d} \cdot \idf{t_1},\;
                \dots \end{bmatrix} \]
\[ \simi{d}{q} =
      \frac{ \vec v_d \cdot \vec v_q }{ \|\vec v_d\| \cdot \|\vec v_q\| } =
      \sum_t \frac{ \tf{t}{d} \cdot \tf{t}{q} \cdot \idf{t}^2 }{ \|\vec v_q\| \cdot \|\vec v_d\| } \]
Tak zdefiniowane $\simi{d}{q}$ uważamy za miarę podobieństwa dokumentu $d$
do zapytania $q$ w modelu przestrzeni wektorowej. Zauważmy, że wartość ta mieści
sie w przedziale $[0, 1]$. Co więcej, jeżeli $\pi_1(d) \cap \pi_1(q) = \emptyset$,
to $\simi{d}{q} = 0$, gdyż $\forall t \;.\; \tf{t}{d} = 0 \vee \tf{t}{q} = 0$.
Zatem podobieństwo dokumentów, które nie zawierają żadnego z termów z zapytania
wynosi zero. Dla pozostałych dokumentów obliczanie powyższej sumy warto wykonywać
jedynie po termach zawartych w zapytaniu, gdyż dla pozostałych $\tf{t}{q} = 0$.

W celu implementacji powyższych rachunków dla każdego dokumentu przechowywana
jest wartość $\|\vec v_d\|$ oraz wartość $|d|$. Ponadto dla każdej pary
(term, dokument), czyli dla każdego wpisu na liście postingowej, przechowywana
jest ilość wystąpień danego termu w dokumencie. Zauważmy, że ilość dokumentów,
które zawierają dany term, to po prostu długość listy postingowej dla danej pary
(term, dokument). Zatem uzupełniając indeks o jedynie trzy wartości (jedną
zmiennopozycyjną i dwie całkowitoliczbowe) jesteśmy w stanie obliczać
podobieństwo w modelu przestrzeni wektorowej.

Na obliczoną wartość $\simi{d}{q}$ nakładana jest jeszcze wartość pagerank
dokumentu $d$. Jako, że wartości pagerank reprezentują prawdopodobieństwo
znalezienia się w danym artykule w wyniku losowego błądzenia, ich wartości 
są niewielkie (od $10^{-3}$ do $10^{-9}$) i zależą od rozmiaru kolekcji
dokumentów. Nie nadają się zatem do prostego złożenia z wartościami podobieństwa
w modelu przestrzeni wektorowej. Dlatego wprowadzono wartość $p(d) = \log (1+|D| \cdot \mathrm{pagerank}(d))$.
Mnożenie przez $|D|$ to dzielenie przez $1/|D|$, czyli średnią wartość pagerank,
przeto wartość $p(d)$ wyraża odchylenie wartości pagerank od średniej.
Ostatecznie jakość artykułu obliczana jest jako
\[ \mathrm{score}(d,q) = \alpha \cdot \simi{d}{q} +
                         \beta \cdot p(d) +
                         \gamma \cdot \simi{d}{q} \cdot p(d) \]
Wartości $\alpha, \beta, \gamma$ wyznaczono eksperymentalnie dla uzyskania
jak najlepszych wyników na $\alpha = ??, \beta = ??, \gamma = ??$. Parametry
te można zmienić uruchamiając wyszukiwarkę z opcją \texttt{-f}.

\section{Budowanie indeksu}
Proces budowania indeksu podzielony jest na kilka etapów; każdy z nich jest
wykonywany przez osobny program, które przekazują sobie dane przy użyciu plików
pośrednich. Pliki te mogą mieć znaczne rozmiary, nie wchodzą one jednak w skład
ostatecznego indeksu. Zarówno pliki pośrednie jak i końcowe umieszczane są w
katalogu \texttt{db/}. Właściwy indeks składa się z następujących czterech plików:
\begin{itemize}
\item \texttt{artitles} (magiczna liczba \texttt{TITL}): plik ten stanowi listę
wszystkie informacji dotyczących artykułów: ich tytuły, ilości termów, wagi w
modelu przestrzeni wektorowej oraz wartości pagerank; 
\item \texttt{dictionary} (magiczna liczba \texttt{DICT}): plik ten jest tablicą
hashującą umożliwiającą dostęp do wszystkich informacji o termach: ich postaci
tekstowych, lokalizacjach list pozycyjnych oraz lematyzowanych oraz wartościach
idf.
\item \texttt{lemmatized} (magiczna liczba \texttt{IDXL}): plik ten zawiera
lematyzowane listy postingowe wszystkich termów; listy te wzbogacone są o
współczynniki umożliwiające obliczanie podobieństwa w modelu przestrzeni wektorowej.
\item \texttt{positional} (magiczna liczba \texttt{IDXP}): plik ten zawiera
pozycyjne listy postingowe wszystkich termów.
\end{itemize}

\paragraph{Tokenizacja Wikipedii} Dostarczone dane wejściowe są zamieniane
na format, w którym każdy artykuł jest zapisany w jednej linii składającej się
z kolejnych słów występujących w artykule oddzielonych spacjami. Na tym etapie
usuwane są wszystkie zbędne znaki, dokonywana jest zamiana liter na małe
oraz formowane są termy. Tokeny, które zawierają znaki różne od liter alfabetu
łacińskiego, rozszerzonego o~ich wszystkie odpowiedniki ze znakami diakrytycznymi,
uznawane są za niepoprawne termy i~zastępowane cudzysłowem ('') w~celu
uwzględnieniach ich pozycji w~tekście. Artykuły ,,ciekawe'' są dodatkowo
oznaczane przez wprowadzenie do nich termu ''m (cudzysłów-m). Dzięki liście
postingowej tego specjalnego termu wyszukiwarka potrafi szybko wyeliminować
nieciekawe artykuły z wyników wyszukiwania. Na tym etapie parsowana jest także
lista linków między artykułami, posłuży ona do obliczenia wartości pagerank. 
Tokenizacja generuje zręb pliku \texttt{TITL}.

\paragraph{Obliczanie pagerank} Na podstawie zparsowanej listy linków obliczane
są wartości pagerank. Trafiają one do utworzonego w poprzednim kroku pliku
\texttt{TITL}. Pagerank obliczany jest przez przyjęcie równych wartości
prawdopodobieństw dla wszystkich dokumentów i iteracyjne obliczanie
$M^{k+1} \vec v = M(M^k \vec v)$. Zbieżność tej metody jest zdecydowanie
wolniejsza od metody $M^{2k} \vec v = (M^k)^2 \vec v$, jednak nie wymaga
potęgowania macierzy, przez co zużywa zdecydowanie mniej pamięci (mnożenie przez
$M$ daje się wykonać w czasie proporcjonalnym do ilości linków; zwykłe
potęgowanie $M$ spowodowałoby powstawie macierzy gęstej, która nie mieści się
w pamięci). W każdym kroku obliczana jest też wartość $\delta = \max_k |v'_k - v_k|/v_k$,
algorytm uznaje otrzymany wektor za ostateczny gdy $\delta < 10^{-4}$. Kontrolnie
obliczana jest też wartość $\sigma = \sum_k v_k$. W każdym kroku powinno być
$\sigma = 1$, jednak ze względu na błędy numeryczne wartość ta może odchylić
się od poprawnej. W praktyce odchylenie to przyjmuje wartość rzędu $10^{-5}$
po ponad 65 krokach algorytmu. Dla wydajności zastosowano arytmetykę
\texttt{float}, w której względny błąd reprezentacji wynosi około $10^{-7}$.
Prawdopodobieństwo teleportacji przyjęto na $10\%$.

\paragraph{Tworzenie korpusu} Na podstawie ztokenizowanej Wikipedii oraz
morfologika tworzony jest korpus. Jest to plik o formacie podobnym do
słownika. Stanowi on tabelę hashującą zawierającą termy Wikipedii oraz
morfologika. Dzięki temu plikowi w dalszej części procesu możliwa będzie
łatwa zamiana słów na liczby. Należy zauważyć, że w pliku tym umieszczane są
wszystkie słowa, z jakimi kiedykolwiek może spotkać się indekser.

\paragraph{Parsowanie morfologika} Przy użyciu korpusu tworzona jest binarna
wersja morfologika: zawiera ona takie same informacje, jak zwykły morfologik
jednak jest prostsza w obsłudze, ponieważ wszystkie termy zamienione zostają
na ich identyfikatory. Ponadto morfologik zostaje uzupełniony o słowa
znajdujące się w korpusie, a nie znajdujące się w oryginalnym morfologiku:
dla słów tych przyjmuje się, że są one swoją postacią bazową.

\paragraph{Wykrywanie aliasów} Na podstawie binarnego morfologika tworzona
jest baza aliasów: dla każdego termu określane jest, czy powinien on mieć
swoją własną listę lematyzowaną, czy też powinien korzystać z listy innego
termu (czyli być aliasem). Jeśli term powinien być aliasem, określane jest,
czyjej listy lematyzowanej powinien używać.

\paragraph{Budowanie grafu lematyzacji} Na podstawie binarnego morfologika
oraz bazy aliasów tworzony jest graf lematyzacji: dla każdego termu budowana
jest lista termów, których listy lematyzowane powinny zawierać jego
wystąpienia. Na listę tą trafiają jedynie te termy, które nie są aliasami.

\paragraph{Digitalizacja} Ztokenizowana Wikipedia zamieniana jest na format
binarny, to znaczy kolejne tokeny zamieniane są na trójki (\texttt{id\_tokena},
\texttt{nr\_dokumentu}, \texttt{nr\_słowa\_w\_dokumencie}), gdzie wartość
\texttt{id\_tokena} wyznaczana jest przez korpus.

\paragraph{Lematyzacja} Na postawie zdigitalizowanej Wikipedii oraz grafu
lematyzacji przeprowadzana jest lematyzacja: każdy term zdigitalizowanej
Wikipedii jest zamieniany na te termy, których listy lematyzowane powinny
złapać jego wystąpienie.

\paragraph{Odwracanie} Zdigitalizowana Wikipedia oraz zlematyzowana Wikipedia
są sortowane leksykograficznie, przez co powstają zaczątki indeksu odwróconego.

\paragraph{Tworzenie indeksów i słownika} Na podstawie korpusu oraz indeksów
odwróconych powstałych w poprzednim kroku powstaje słownik (plik
\texttt{DICT}), indeks pozycyjny (plik \texttt{IDXP}) oraz indeks zlematyzowany
(plik \texttt{IDXL}). Na tym etapie usuwane są słowa, które nie mają żadnych
postingów (mogły one znaleźć się w korpusie, gdyż występowały w morfologiku,
jednak żaden term Wikipedii nie ,,aktywował'' ich). Ponadto obliczane są
wartości idf termów oraz inne współczynniki niezbędne dla realizacji zapytań w
modelu przestrzeni wektorowej.

\paragraph{Oznaczanie stopwords} W gotowych plikach indeksu na każde
\textit{stopword} nanoszony jest specjalny znacznik, który jest rozpoznawany
przez wyszukiwarkę.

\section{Pomiary czasu działania}
Pomiary czasu zostały przeprowadzone na laptopie wyposażonym w:
\begin{itemize}
  \item procesor: Core 2 Duo 2.0 GHz, 4 MB cache
  \item pamięć: 2 GB DDR2 667 MHz
  \item dysk: 5400RPM, 8 MB cache
  \item system operacyjny: Gentoo
  \item platforma: \texttt{x86\_64-pc-linux-gnu}
\end{itemize}
Każdy wynik stanowi średnią arytmetyczną całkowitego czasu działania programu
z~trzech pomiarów, zaokrągloną do sekund.
\footnote{do pomiaru czasu użyty został program \texttt{GNU time}}

Przed każdym uruchomieniem wyszukiwarki wykonywane było polecenie:
\begin{verbatim}
echo 3 > /proc/sys/vm/drop_caches
\end{verbatim}
w~celu wyczyszczenia systemowego buforu. Dzięki powyższemu wszystkie czasy zawarte
w~raporcie uwzględniają całkowity rozruch wyszukiwarki. Dodatkowym plusem
wyczyszczenia bufora podręcznego dla plików jest poprawa wiarygodności
pomiarów, które w~przeciwnym wypadku byłyby zaburzona za sprawą plików
zbuforowanych podczas poprzednich wyszukiwań.

\paragraph{Tokenizacja}
Pierwszym krokiem na drodze do wygenerowania wszystkich, niezbędnych dla
wyszukiwarki, indeksów jest tokenizacja pliku Wikipedii. Jej całkowity czas
działania nieznacznie przekracza \textbf{??} minut, co wynika z~faktu, iż wymagane jest
dekodowanie pliku do \textit{UTF-8} w~celu prawidłowego rozpoznawania
wszystkich znaków. Czas tokenizacji Wikipedii na komputerze z~pracowni numer 7
(Intel Core i3 CPU 3.07GHz, 4GB ramu, Ubuntu, i686-linux-gnu) wynosi około \textbf{??}
minut.

\paragraph{Generowanie indeksów}
Ta część przetwarzania ztokenizowanej Wikipedii zajmuje niewiele ponad \textbf{??}
minut, z~czego najwięcej czasu zabiera proces odwracania, czyli
sortowania tymczasowych list postingowych zdigitalizowanej Wikipedii. Czas
generowania indeksów jest zdecydowanie mniejszy na komputerach z~pracowni numer
7 -- około \textbf{??} minut -- czego główną przyczyną jest spora ilość pamięci RAM,
która umożliwia systemowi operacyjnemu efektywne buforowanie plików pośrednich.

\paragraph{Czas rozruchu}
Czas potrzebny wyszukiwarce na rozruch, wynoszący około \textbf{??} sekundy, wynika
głównie z~potrzebny wczytania plików zawierających listę wszystkich tytułów
artykułów oraz listę wszystkich słów pozyskanych z~Wikipedii i~morfologika wraz
z~długościami, a~także pozycjami w~plikach indeksów, ich list postingowych
i~lematyzacyjnych\footnote{pliki \texttt{db/artitles} -- 24MB,
\texttt{db/dictionary} - 71MB}. Wyszukiwarka podczas działania zajmuje około
300MB pamięci operacyjnej.

\section{Ocena mechanizmu rankingowania dokumentów}
\textbf{??}

\end{document}
